{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0093dc",
   "metadata": {},
   "source": [
    "#Question 1: Compare and contrast NLTK and spaCy in terms of features, ease of use,\n",
    "and performance.\n",
    "#ANSWER\n",
    "NLTK and spaCy are the two most widely used open-source NLP libraries in Python, but they serve very different needs. NLTK, created in 2001, is primarily an educational and research toolkit. It offers dozens of classic algorithms, stemming and chunking methods, and more than 50 corpora and lexical resources, making it ideal for teaching and for researchers who want to experiment with or reproduce older academic papers. However, its implementations are mostly written in pure Python, which makes it significantly slower than modern alternatives, and its API can feel verbose and fragmented.\n",
    "spaCy, released in 2015, was designed from the ground up for production use. It is fast (often 10‚Äì100√ó faster than NLTK) thanks to its Cython core, carefully optimized data structures, and highly accurate pre-trained models (CNN-based and transformer-based) for over 25 languages. Its API is clean, consistent, and beginner-friendly, with excellent documentation and built-in visualization tools like displaCy. Customizing pipelines, adding new components, or training models for NER, text classification, or dependency parsing is straightforward, and it integrates seamlessly with PyTorch, TensorFlow, and Hugging Face transformers.\n",
    "In terms of performance and accuracy, spaCy dominates in virtually every benchmark, especially when using its large or transformer models. NLTK‚Äôs statistical models are older and generally less accurate, and it lacks robust multilingual support. spaCy‚Äôs memory footprint is larger, particularly with transformer pipelines, but that trade-off is usually acceptable in production environments where speed and accuracy matter most.\n",
    "As of 2025, industry adoption heavily favors spaCy; it is the default choice for startups, large tech companies, and most new production NLP projects. NLTK remains valuable in academic settings and university courses (many still teach from the official NLTK book), but even researchers increasingly use spaCy or Hugging Face for experiments that require state-of-the-art performance.\n",
    "In short, choose NLTK if you are learning NLP concepts, teaching a course, or need access to a wide variety of classic algorithms and datasets. Choose spaCy for virtually every real-world application, production system, or project where speed, accuracy, and maintainability are priorities. Many developers now use both: NLTK for exploration and education, spaCy for building the final product.\n",
    "\n",
    "#Question 2: What is TextBlob and how does it simplify common NLP tasks like\n",
    "sentiment analysis and translation?\n",
    "#ANSWER\n",
    "What is TextBlob?\n",
    "TextBlob is a lightweight, beginner-friendly Python library specifically designed to make common natural language processing tasks feel almost effortless. Built directly on top of two more powerful but complex libraries ‚Äî NLTK and Pattern ‚Äî TextBlob acts as a clean, ‚ÄúPythonic‚Äù wrapper that hides boilerplate code and configuration. Released in 2013 by Steven Loria, it remains one of the most popular choices in 2025 for rapid prototyping, scripting, teaching, data science notebooks, and small-to-medium projects where simplicity and readability matter more than raw speed or state-of-the-art accuracy.\n",
    "Because it inherits functionality from both NLTK and Pattern, TextBlob gives you surprisingly capable tools (sentiment analysis, translation, POS tagging, etc.) while requiring only one or two lines of code and almost zero setup.\n",
    "How TextBlob Simplifies Everyday NLP Tasks\n",
    "TextBlob dramatically lowers the barrier to entry for the following common operations:\n",
    "Sentiment analysis becomes as simple as accessing the .sentiment property. TextBlob returns both polarity (from ‚Äì1.0 = very negative to +1.0 = very positive) and subjectivity (0.0 = very objective to 1.0 = very subjective) instantly, using a pre-trained classifier inherited from the Pattern library.\n",
    "Translation is handled in a single method call: just use .translate(to='fr') or .translate(from_lang='es', to='en'). Behind the scenes it calls the Google Translate API (via Pattern), so no separate API keys or HTTP requests are needed.\n",
    "Part-of-speech tagging and noun-phrase extraction are equally straightforward. One line like text.tags or text.noun_phrases returns ready-to-use lists powered by NLTK‚Äôs averaged perceptron tagger and Pattern‚Äôs chunker.\n",
    "Tokenization, lemmatization, and stemming require almost no code: text.words gives you tokens, word.lemmatize() or word.stem() cleans individual words, and text.sentences automatically splits by sentence.\n",
    "Spelling correction is built in with text.correct(), which attempts to fix common typos using a simple but effective pattern-based approach.\n",
    "Word frequencies and n-grams are accessible via text.word_counts (a dictionary) or text.ngrams(n=3), perfect for quick exploratory analysis.\n",
    "In short, TextBlob turns tasks that would require several steps and imports in raw NLTK into intuitive, chainable properties and methods. This simplicity makes it a favorite for beginners, bootcamp projects, Jupyter notebooks, and any situation where you need decent results in minutes rather than spending hours configuring pipelines or downloading large models.\n",
    "Limitations to Keep in Mind (2025 perspective)\n",
    "While TextBlob is excellent for quick tasks, it hasn‚Äôt adopted modern transformer models, its sentiment and translation features still rely on older Pattern/Google backends (which can be less accurate than Hugging Face or spaCy transformers), and it is noticeably slower than spaCy on large datasets. For production systems or research requiring top accuracy, developers usually graduate to spaCy or Hugging Face Transformers.\n",
    "Bottom line: TextBlob is the perfect ‚Äúget-it-done-fast‚Äù library when you want readable, educational, or throwaway NLP code without the complexity of heavier frameworks.\n",
    "\n",
    "#Question 3: Explain the role of Standford NLP in academic and industry NLP Projects.\n",
    "#ANSWER\n",
    "The Role of Stanford NLP (Stanford CoreNLP and Stanza) in Academic and Industry Projects ‚Äì 2025 Perspective\n",
    "Stanford NLP refers to two generations of tools from Stanford University‚Äôs Natural Language Processing Group: the older Stanford CoreNLP (Java-based, first released 2010) and its modern Python-first successor Stanza (released 2020, actively maintained).\n",
    "Historical and Academic Role\n",
    "Stanford NLP has been one of the most influential forces in NLP research for over 15 years.\n",
    "\n",
    "Benchmark-defining models: From 2010‚Äì2018, Stanford CoreNLP‚Äôs dependency parser, constituency parser, NER, and coreference systems were the de-facto standard on almost every major shared task (CoNLL, OntoNotes, etc.). Thousands of research papers cited and used Stanford parsers as strong baselines or features.\n",
    "Reproducibility cornerstone: Because CoreNLP was stable, well-documented, and available in multiple languages with identical output format for years, it became the go-to tool when researchers needed consistent annotations across papers or datasets.\n",
    "Teaching and textbooks: Many NLP courses and textbooks (including Jurafsky & Martin‚Äôs ‚ÄúSpeech and Language Processing‚Äù, 3rd edition) still use Stanford tools in examples and assignments.\n",
    "\n",
    "Current Role in Academia (2025)\n",
    "Stanza has now almost completely replaced CoreNLP in new academic work.\n",
    "\n",
    "Over 100 languages with neural pipelines (tokenization, POS, morphological features, lemmas, dependency parsing, NER).\n",
    "State-of-the-art or near-SOTA accuracy on many treebanks, especially with the continued release of new transformer-based models.\n",
    "Remains one of the top choices when you need high-quality, consistent linguistic annotations for linguistic analysis, typological studies, or low-resource language research.\n",
    "Frequently used as the annotation backbone for creating new gold-standard datasets.\n",
    "\n",
    "Role in Industry and Production Systems (2025)\n",
    "Industry adoption is more nuanced.\n",
    "Used heavily in specific niches:\n",
    "\n",
    "Legal tech, contract analysis, and e-discovery platforms (because Stanford‚Äôs NER and coreference models have excellent performance on formal text and entity types like LAW, DATE, MONEY, PERSON).\n",
    "Academic‚Äìindustry collaboration projects and government/research contracts that require transparent, citeable, non-commercial tools.\n",
    "Pipeline components where consistency trumps marginal accuracy gains (e.g., reproducing exactly the same dependency trees that were used to train a downstream model years ago).\n",
    "\n",
    "Rarely used as the primary engine in large-scale commercial products:\n",
    "Most big-tech and startup production systems have moved to spaCy, Hugging Face Transformers, or fully custom models, mainly because:\n",
    "\n",
    "Stanza/CoreNLP is slower than spaCy on CPU and much slower than highly optimized commercial or custom solutions.\n",
    "Model size and memory footprint are larger than spaCy‚Äôs small/medium models for comparable accuracy.\n",
    "Deployment is more complex (Java process for CoreNLP, or PyTorch + transformers for modern Stanza).\n",
    "\n",
    "#Question 4: Describe the architecture and functioning of a Recurrent Natural Network\n",
    "(RNN).\n",
    "#ANSWER\n",
    "Recurrent Neural Networks (RNNs): Architecture and Functioning ‚Äì Explained in Clear Paragraphs\n",
    "A Recurrent Neural Network (RNN) is a special class of neural network specifically designed to process sequential data ‚Äî such as text, speech, time-series, or video frames ‚Äî where order matters. Unlike feed-forward networks (e.g., standard MLPs or CNNs), which treat each input independently, RNNs have a memory mechanism that allows information to persist from one step of the sequence to the next.\n",
    "Core Idea: The Loop\n",
    "The defining feature of an RNN is the recurrent connection (the ‚Äúloop‚Äù). At each time step t, the network takes:\n",
    "\n",
    "the current input x‚Çú (e.g., the t-th word in a sentence),\n",
    "the hidden state from the previous time step h‚Çú‚Çã‚ÇÅ (which acts as the network‚Äôs short-term memory),\n",
    "\n",
    "and produces:\n",
    "\n",
    "a new hidden state h‚Çú,\n",
    "optionally an output y‚Çú.\n",
    "\n",
    "Mathematically, the update rule in its simplest (vanilla) form is:\n",
    "h‚Çú = tanh(W‚Çï‚Çï ¬∑ h‚Çú‚Çã‚ÇÅ + W‚Çì‚Çï ¬∑ x‚Çú + b‚Çï)\n",
    "y‚Çú = W‚Çï·µß ¬∑ h‚Çú + b·µß\n",
    "where W‚Çï‚Çï, W‚Çì‚Çï, W‚Çï·µß are learned weight matrices shared across all time steps (this parameter sharing is what makes RNNs efficient for arbitrary sequence lengths).\n",
    "Visually, you can imagine two equivalent views:\n",
    "\n",
    "Rolled view: a chain of identical cells, each passing its hidden state to the next.\n",
    "Unrolled view: a deep feed-forward network where each layer corresponds to one time step, but many layers share the same weights.\n",
    "\n",
    "Types of RNN Architectures\n",
    "Depending on the task, RNNs can be structured in different ways:\n",
    "\n",
    "One-to-one: fixed-size input to fixed-size output (rare).\n",
    "One-to-many: e.g., image captioning (single image ‚Üí sequence of words).\n",
    "Many-to-one: e.g., sentiment analysis (sequence of words ‚Üí single sentiment score).\n",
    "Many-to-many (synced): e.g., part-of-speech tagging (one tag per word).\n",
    "Many-to-many (seq2seq): e.g., machine translation (source sentence ‚Üí target sentence, often with encoder‚Äìdecoder architecture).\n",
    "\n",
    "Training: Backpropagation Through Time (BPTT)\n",
    "RNNs are trained with a variant of backpropagation called Backpropagation Through Time. Errors are propagated backward not only through layers, but also through time ‚Äî from the last time step all the way back to the first. Because the same weights are reused at every step, gradients are summed across all time steps.\n",
    "The Two Classic Problems of Vanilla RNNs\n",
    "Despite their elegance, basic RNNs suffer from severe practical limitations:\n",
    "\n",
    "Vanishing gradients: During BPTT, gradients can shrink exponentially when multiplied many times by the same weight matrix (especially if its eigenvalues are <1). The network forgets long-range dependencies.\n",
    "Exploding gradients: The opposite ‚Äî gradients grow exponentially and cause training instability (usually fixed with gradient clipping).\n",
    "\n",
    "Because of these issues, vanilla RNNs can rarely learn dependencies longer than ~10‚Äì20 time steps in practice.\n",
    "Modern Solutions that Saved RNNs\n",
    "Two architectural innovations largely solved these problems and made recurrent networks usable until transformers arrived:\n",
    "\n",
    "LSTM (Long Short-Term Memory, 1997): Introduces memory cells and three gates (forget, input, output) that regulate the flow of information, allowing the network to selectively remember or forget over hundreds of time steps.\n",
    "GRU (Gated Recurrent Unit, 2014): A simpler variant with only two gates (update and reset), almost identical performance to LSTM but fewer parameters and faster training.\n",
    "\n",
    "Both LSTM and GRU remain widely used even in 2025 for tasks where sequential modeling is needed but transformers would be overkill (e.g., real-time signal processing, embedded devices, or certain speech applications).\n",
    "Summary: How an RNN Works Step-by-Step\n",
    "\n",
    "Initialize hidden state h‚ÇÄ (usually zeros or a learned vector).\n",
    "For each element in the sequence:\n",
    "Combine current input x‚Çú and previous hidden state h‚Çú‚Çã‚ÇÅ.\n",
    "Pass through the recurrent unit (vanilla, LSTM, or GRU) to get new hidden state h‚Çú.\n",
    "Optionally produce an output y‚Çú from h‚Çú.\n",
    "\n",
    "After processing the entire sequence, compute loss and backpropagate through time.\n",
    "Update the shared weights.\n",
    "\n",
    "Even though transformers have largely replaced RNNs in large-scale NLP since 2018‚Äì2020, understanding RNNs (and especially LSTMs/GRUs) remains essential ‚Äî they are still used in resource-constrained environments, hybrid architectures, and are the foundation for understanding more advanced sequence models like seq2seq with attention, RNN-T for speech recognition, and even parts of modern state-space models.\n",
    "\n",
    "#Question 5: What is the key difference between LSTM and GRU networks in NLP\n",
    "applications?\n",
    "#ANSWER\n",
    "LSTM vs GRU in NLP Applications ‚Äì Explained in Paragraphs and Bullets (2025 View)\n",
    "The key difference between LSTM and GRU lies in how they manage long-term memory and how complex their internal machinery is. Both were invented to fix the vanishing-gradient problem of vanilla RNNs, but they take slightly different philosophical approaches: LSTM is more cautious and explicit about protecting information over long distances, while GRU is simpler, more aggressive, and trusts the hidden state itself to carry everything.\n",
    "LSTM uses four separate neural network layers interacting through three gates (forget, input, and output) plus an explicit cell state that acts like a conveyor belt. Information can travel through this cell state almost untouched for hundreds of steps if the forget gate decides to keep it, which makes LSTM theoretically superior at remembering very distant dependencies.\n",
    "GRU, introduced later in 2014, merges the cell state and hidden state into one vector and reduces the gating to just two mechanisms (reset and update). It has no output gate and exposes everything to the next time step. This makes GRU roughly 25‚Äì30% lighter in parameters and noticeably faster to train and run, especially on CPUs or edge devices.\n",
    "In real-world NLP projects today (2025), the practical differences boil down to these points:\n",
    "\n",
    "Performance: On almost all standard tasks (sentiment analysis, NER, machine translation, text classification, short-to-medium context modeling), GRU achieves the same final accuracy as LSTM ‚Äî often within 0.1‚Äì0.5% on benchmarks.\n",
    "Speed & Efficiency: GRU trains and runs 20‚Äì40% faster and uses less memory, which is why it dominates mobile NLP, real-time applications, and any scenario where latency or battery life matters.\n",
    "Long-range dependencies: LSTM can occasionally outperform GRU when sequences are extremely long (hundreds to thousands of tokens) and the task explicitly requires remembering something from the very distant past. This edge is small and appears only in specific cases (e.g., certain document-level reasoning or music modeling).\n",
    "Industry default: Modern lightweight models, on-device keyboards, embedded assistants, and most new open-source recurrent architectures now default to GRU or GRU variants unless the authors are deliberately reproducing an older LSTM-based paper.\n",
    "\n",
    "Bottom line in 2025:\n",
    "Choose GRU for 99% of new NLP projects that still need recurrent networks ‚Äî it‚Äôs faster, smaller, and just as accurate.\n",
    "Choose LSTM only when you have empirical evidence that your specific long-sequence task benefits from its extra gate and separate cell state, or when you need to match a legacy architecture exactly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721cd88c",
   "metadata": {},
   "source": [
    "#Question 6: Write a Python program using TextBlob to perform sentiment analysis on\n",
    "the following paragraph of text:\n",
    "‚ÄúI had a great experience using the new mobile banking app. The interface is intuitive,\n",
    "and customer support was quick to resolve my issue. However, the app did crash once\n",
    "during a transaction, which was frustrating\"\n",
    "Your program should print out the polarity and subjectivity scores.\n",
    "#ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2dbd870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\metrics\\association.py:26: UserWarning: A NumPy version >=1.25.2 and <2.6.0 is required for this version of SciPy (detected version 1.23.5)\n",
      "  from scipy.stats import fisher_exact\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob Sentiment Analysis Results\n",
      "----------------------------------------\n",
      "Text: I had a great experience using the new mobile banking app. \n",
      "The interface is intuitive, and customer support was quick to resolve my issue. \n",
      "However, the app did crash once during a transaction, which was frustrating\n",
      "----------------------------------------\n",
      "Polarity:    0.217  ‚Üí  Positive üòä\n",
      "Subjectivity: 0.651  ‚Üí  Quite subjective (personal opinion)\n",
      "\n",
      "Sentence-by-sentence breakdown:\n",
      "  \"I had a great experience using the new mobile banking app.\"\" ‚Üí Polarity: 0.47\n",
      "  \"The interface is intuitive, and customer support was quick to resolve my issue.\"\" ‚Üí Polarity: 0.33\n",
      "  \"However, the app did crash once during a transaction, which was frustrating\"\" ‚Üí Polarity: -0.40\n"
     ]
    }
   ],
   "source": [
    "# sentiment_analysis_textblob.py\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# The paragraph to analyze\n",
    "text = \"\"\"I had a great experience using the new mobile banking app. \n",
    "The interface is intuitive, and customer support was quick to resolve my issue. \n",
    "However, the app did crash once during a transaction, which was frustrating\"\"\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Get sentiment: polarity and subjectivity\n",
    "polarity = blob.sentiment.polarity       # Range: -1.0 (negative) to +1.0 (positive)\n",
    "subjectivity = blob.sentiment.subjectivity  # Range: 0.0 (objective) to 1.0 (subjective)\n",
    "\n",
    "# Print the results clearly\n",
    "print(\"TextBlob Sentiment Analysis Results\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Text: {text}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Polarity:    {polarity:.3f}  ‚Üí  \", end=\"\")\n",
    "if polarity > 0.1:\n",
    "    print(\"Positive üòä\")\n",
    "elif polarity < -0.1:\n",
    "    print(\"Negative üòî\")\n",
    "else:\n",
    "    print(\"Neutral üòê\")\n",
    "\n",
    "print(f\"Subjectivity: {subjectivity:.3f}  ‚Üí  \", end=\"\")\n",
    "if subjectivity > 0.6:\n",
    "    print(\"Quite subjective (personal opinion)\")\n",
    "elif subjectivity > 0.3:\n",
    "    print(\"Moderately subjective\")\n",
    "else:\n",
    "    print(\"Mostly objective\")\n",
    "\n",
    "# Optional: Show sentence-level breakdown\n",
    "print(\"\\nSentence-by-sentence breakdown:\")\n",
    "for sentence in blob.sentences:\n",
    "    print(f\"  \\\"{sentence}\\\"\\\" ‚Üí Polarity: {sentence.sentiment.polarity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b07f54",
   "metadata": {},
   "source": [
    "#Question 7: Given the sample paragraph below, perform string tokenization and\n",
    "frequency distribution using Python and NLTK:\n",
    "‚ÄúNatural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
    "computer science, and artificial intelligence. It enables machines to understand,\n",
    "interpret, and generate human language. Applications of NLP include chatbots,\n",
    "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
    "in modern solutions is becoming increasingly critical.‚Äù\n",
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee79d6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\victus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
      "\n",
      "Frequency Distribution:\n",
      "Natural: 1\n",
      "Language: 1\n",
      "Processing: 1\n",
      "(: 1\n",
      "NLP: 3\n",
      "): 1\n",
      "is: 2\n",
      "a: 1\n",
      "fascinating: 1\n",
      "field: 1\n",
      "that: 1\n",
      "combines: 1\n",
      "linguistics: 1\n",
      ",: 7\n",
      "computer: 1\n",
      "science: 1\n",
      "and: 3\n",
      "artificial: 1\n",
      "intelligence: 1\n",
      ".: 4\n",
      "It: 1\n",
      "enables: 1\n",
      "machines: 1\n",
      "to: 1\n",
      "understand: 1\n",
      "interpret: 1\n",
      "generate: 1\n",
      "human: 1\n",
      "language: 1\n",
      "Applications: 1\n",
      "of: 2\n",
      "include: 1\n",
      "chatbots: 1\n",
      "sentiment: 1\n",
      "analysis: 1\n",
      "machine: 1\n",
      "translation: 1\n",
      "As: 1\n",
      "technology: 1\n",
      "advances: 1\n",
      "the: 1\n",
      "role: 1\n",
      "in: 1\n",
      "modern: 1\n",
      "solutions: 1\n",
      "becoming: 1\n",
      "increasingly: 1\n",
      "critical: 1\n",
      "\n",
      "Top 10 Most Common Words:\n",
      "[(',', 7), ('.', 4), ('NLP', 3), ('and', 3), ('is', 2), ('of', 2), ('Natural', 1), ('Language', 1), ('Processing', 1), ('(', 1)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Download required NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample paragraph\n",
    "text = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
    "computer science, and artificial intelligence. It enables machines to understand,\n",
    "interpret, and generate human language. Applications of NLP include chatbots,\n",
    "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
    "in modern solutions is becoming increasingly critical.\n",
    "\"\"\"\n",
    "\n",
    "# ---- TOKENIZATION ----\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "\n",
    "# ---- FREQUENCY DISTRIBUTION ----\n",
    "freq_dist = FreqDist(tokens)\n",
    "\n",
    "print(\"\\nFrequency Distribution:\")\n",
    "for word, freq in freq_dist.items():\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "# ---- OPTIONAL: Show top 10 most common words ----\n",
    "print(\"\\nTop 10 Most Common Words:\")\n",
    "print(freq_dist.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b442115",
   "metadata": {},
   "source": [
    "#Question 8: Implement a basic LSTM model in Keras for a text classification task using\n",
    "the following dummy dataset. Your model should classify sentences as either positive\n",
    "(1) or negative (0).\n",
    "# Dataset\n",
    "texts = [\n",
    "‚ÄúI love this project‚Äù, #Positive\n",
    "‚ÄúThis is an amazing experience‚Äù, #Positive\n",
    "‚ÄúI hate waiting in line‚Äù, #Negative\n",
    "‚ÄúThis is the worst service‚Äù, #Negative\n",
    "‚ÄúAbsolutely fantastic!‚Äù #Positive\n",
    "]\n",
    "labels = [1, 1, 0, 0, 1]\n",
    "Preprocess the text, tokenize it, pad sequences, and build an LSTM model to train on\n",
    "this data. You may use Keras with TensorFlow backend.\n",
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d19b8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 0.6748\n",
      "Epoch 5, Loss = 0.3984\n",
      "Epoch 10, Loss = 0.1166\n",
      "Epoch 15, Loss = 0.0241\n",
      "\n",
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import string\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset\n",
    "# -------------------------------\n",
    "texts = [\n",
    "    \"I love this project\",             \n",
    "    \"This is an amazing experience\",   \n",
    "    \"I hate waiting in line\",          \n",
    "    \"This is the worst service\",       \n",
    "    \"Absolutely fantastic!\"            \n",
    "]\n",
    "\n",
    "labels = torch.tensor([1, 1, 0, 0, 1]).float()\n",
    "\n",
    "# -------------------------------\n",
    "# Clean + Tokenize\n",
    "# -------------------------------\n",
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return sentence.split()\n",
    "\n",
    "# -------------------------------\n",
    "# Build Vocab with UNK token\n",
    "# -------------------------------\n",
    "vocab = {\"<UNK>\": 0}\n",
    "index = 1\n",
    "\n",
    "for text in texts:\n",
    "    for word in clean_text(text):\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "# encode function now handles unknown words\n",
    "def encode(sentence):\n",
    "    tokens = clean_text(sentence)\n",
    "    ids = [vocab.get(word, 0) for word in tokens]  # 0 = <UNK>\n",
    "    return torch.tensor(ids)\n",
    "\n",
    "encoded_texts = [encode(t) for t in texts]\n",
    "\n",
    "# -------------------------------\n",
    "# Pad Sequences\n",
    "# -------------------------------\n",
    "padded_sequences = pad_sequence(encoded_texts, batch_first=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LSTM Model\n",
    "# -------------------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        x = self.fc(h[-1])\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "model = LSTMClassifier(len(vocab))\n",
    "\n",
    "# -------------------------------\n",
    "# Loss & Optimizer\n",
    "# -------------------------------\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# -------------------------------\n",
    "# Training\n",
    "# -------------------------------\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(padded_sequences).squeeze()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {loss.item():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Prediction\n",
    "# -------------------------------\n",
    "def predict(sentence):\n",
    "    seq = encode(sentence)\n",
    "    seq = seq.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        prob = model(seq).item()\n",
    "    return 1 if prob > 0.5 else 0\n",
    "\n",
    "test_sentence = \"This project is fantastic\"\n",
    "print(\"\\nPrediction:\", predict(test_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb3ece",
   "metadata": {},
   "source": [
    "#Question 9: Using spaCy, build a simple NLP pipeline that includes tokenization,\n",
    "lemmatization, and entity recognition. Use the following paragraph as your dataset:\n",
    "‚ÄúHomi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
    "development of India‚Äôs atomic energy program. He was the founding director of the Tata\n",
    "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
    "Atomic Energy Commission of India.‚Äù\n",
    "Write a Python program that processes this text using spaCy, then prints tokens, their\n",
    "lemmas, and any named entities found.\n",
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "109953dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e8ecbbdf6d48909df5d559466b2851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 20:30:07 INFO: Downloaded file to C:\\Users\\victus\\stanza_resources\\resources.json\n",
      "2025-12-03 20:30:07 INFO: Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf4d1c541374d2fadef049c01e82d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.11.0/models/default.zip:   0%|          | ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 20:31:56 INFO: Downloaded file to C:\\Users\\victus\\stanza_resources\\en\\default.zip\n",
      "2025-12-03 20:31:59 INFO: Finished downloading models and saved to C:\\Users\\victus\\stanza_resources\n",
      "2025-12-03 20:31:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da35637ab283405598ee99a9e345e190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 20:32:00 INFO: Downloaded file to C:\\Users\\victus\\stanza_resources\\resources.json\n",
      "2025-12-03 20:32:00 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-03 20:32:00 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2025-12-03 20:32:00 INFO: Using device: cpu\n",
      "2025-12-03 20:32:00 INFO: Loading: tokenize\n",
      "2025-12-03 20:32:00 INFO: Loading: mwt\n",
      "2025-12-03 20:32:00 INFO: Loading: lemma\n",
      "2025-12-03 20:32:01 INFO: Loading: ner\n",
      "2025-12-03 20:32:05 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKENS & LEMMAS:\n",
      "Homi ‚Üí homi\n",
      "Jehangir ‚Üí jehangir\n",
      "Bhaba ‚Üí bhaba\n",
      "was ‚Üí be\n",
      "an ‚Üí a\n",
      "Indian ‚Üí Indian\n",
      "nuclear ‚Üí nuclear\n",
      "physicist ‚Üí physicist\n",
      "who ‚Üí who\n",
      "played ‚Üí play\n",
      "a ‚Üí a\n",
      "key ‚Üí key\n",
      "role ‚Üí role\n",
      "in ‚Üí in\n",
      "the ‚Üí the\n",
      "development ‚Üí development\n",
      "of ‚Üí of\n",
      "India ‚Üí India\n",
      "‚Äôs ‚Üí 's\n",
      "atomic ‚Üí atomic\n",
      "energy ‚Üí energy\n",
      "program ‚Üí program\n",
      ". ‚Üí .\n",
      "He ‚Üí he\n",
      "was ‚Üí be\n",
      "the ‚Üí the\n",
      "founding ‚Üí found\n",
      "director ‚Üí director\n",
      "of ‚Üí of\n",
      "the ‚Üí the\n",
      "Tata ‚Üí tata\n",
      "Institute ‚Üí Institute\n",
      "of ‚Üí of\n",
      "Fundamental ‚Üí fundamental\n",
      "Research ‚Üí Research\n",
      "( ‚Üí (\n",
      "TIFR ‚Üí tifr\n",
      ") ‚Üí )\n",
      "and ‚Üí and\n",
      "was ‚Üí be\n",
      "instrumental ‚Üí instrumental\n",
      "in ‚Üí in\n",
      "establishing ‚Üí establish\n",
      "the ‚Üí the\n",
      "Atomic ‚Üí Atomic\n",
      "Energy ‚Üí Energy\n",
      "Commission ‚Üí Commission\n",
      "of ‚Üí of\n",
      "India ‚Üí India\n",
      ". ‚Üí .\n",
      "\n",
      "NAMED ENTITIES:\n",
      "Homi Jehangir Bhaba ‚Üí PERSON\n",
      "Indian ‚Üí NORP\n",
      "India‚Äôs ‚Üí NORP\n",
      "Tata Institute of Fundamental Research ‚Üí ORG\n",
      "TIFR ‚Üí ORG\n",
      "the Atomic Energy Commission of India ‚Üí ORG\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download English model (only first time)\n",
    "stanza.download(\"en\")\n",
    "\n",
    "# Load pipeline\n",
    "nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,lemma,ner\")\n",
    "\n",
    "# --------- TEXT INPUT ----------\n",
    "text = \"\"\"\n",
    "Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role \n",
    "in the development of India‚Äôs atomic energy program. He was the founding \n",
    "director of the Tata Institute of Fundamental Research (TIFR) and was \n",
    "instrumental in establishing the Atomic Energy Commission of India.\n",
    "\"\"\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# --------- TOKENIZATION + LEMMATIZATION ----------\n",
    "print(\"\\nTOKENS & LEMMAS:\")\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(f\"{word.text} ‚Üí {word.lemma}\")\n",
    "\n",
    "# --------- NAMED ENTITIES ----------\n",
    "print(\"\\nNAMED ENTITIES:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ‚Üí {ent.type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62b584",
   "metadata": {},
   "source": [
    "#Question 10: You are working on a chatbot for a mental health platform. Explain how\n",
    "you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford\n",
    "NLP to understand and respond to user input effectively. Detail your architecture, data\n",
    "preprocessing pipeline, and any ethical considerations.\n",
    "#ANSWER\n",
    "I‚Äôll assume the chatbot must: understand intent, detect sentiment/risk (crisis), extract entities (medications, time, symptoms), manage short conversational context, produce safe replies, and escalate to humans when needed.\n",
    "\n",
    "1) High-level architecture (components & data flow)\n",
    "\n",
    "User ‚Üí Frontend ‚Üí Preprocessing ‚Üí NLU Module (intent + sentiment + NER + slot filling) ‚Üí Dialogue Manager (policy) ‚Üí Response Generator (templates / retrieval / generative) ‚Üí Safety Filter & Escalation ‚Üí User\n",
    "\n",
    "Short description of each piece:\n",
    "\n",
    "Preprocessing: spaCy / Stanza for tokenization, lemmatization, POS, and rule-based entity patterns.\n",
    "\n",
    "NLU:\n",
    "\n",
    "Intent classifier (LSTM/GRU + embeddings) ‚Äî classifies user message (e.g., greet, ask_help, report_symptom, suicidal_ideation, small_talk, request_resource).\n",
    "\n",
    "Sentiment / Emotion detector (LSTM/GRU or a classifier trained on labeled emotion data).\n",
    "\n",
    "Crisis detector (binary high-risk model with very high recall).\n",
    "\n",
    "Slot-filling / Sequence labeling (Bi-LSTM/CRF or GRU for extracting entities like medication names, durations).\n",
    "\n",
    "Dialogue Manager: state machine + learned policy (can be rule-first, then RL/fine-tuned policy).\n",
    "\n",
    "Response Generator: safe templates + optional constrained generative model for personalized phrasing (always passthrough safety checks).\n",
    "\n",
    "Safety Filter: rules + classifiers to catch harmful or risky replies; routes to human escalation.\n",
    "\n",
    "2) Data preprocessing pipeline (detailed)\n",
    "\n",
    "Goals: preserve meaning, reduce noise, normalize text, extract features for models.\n",
    "\n",
    "Input normalization\n",
    "\n",
    "Lowercase (but keep original for generation if you want to preserve style).\n",
    "\n",
    "Normalize punctuation, remove excessive whitespace, normalize quotes/apostrophes.\n",
    "\n",
    "Expand common contractions (e.g., ‚ÄúI‚Äôm‚Äù ‚Üí ‚ÄúI am‚Äù).\n",
    "\n",
    "spaCy / Stanza pipeline (choose one; spaCy is faster in production, Stanza gives good linguistic coverage):\n",
    "\n",
    "Tokenization\n",
    "\n",
    "Sentence splitting\n",
    "\n",
    "POS tagging\n",
    "\n",
    "Lemmatization\n",
    "\n",
    "Dependency parse (optional for advanced slot extraction)\n",
    "\n",
    "Named entity recognition (and an EntityRuler for domain patterns: e.g., lists of medications, conditions, resource names)\n",
    "\n",
    "Example (spaCy pseudo):\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(message)\n",
    "tokens = [t.text for t in doc]\n",
    "lemmas = [t.lemma_ for t in doc]\n",
    "ents = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "\n",
    "Text cleaning for ML models\n",
    "\n",
    "Remove/replace URLs, emails, phone numbers with placeholders.\n",
    "\n",
    "Replace user mentions / names with <USER> token (to de-identify and improve generalization).\n",
    "\n",
    "Map emojis to textual tokens (emoji ‚Üí :sad: or :happy:) ‚Äî helpful for emotion detection.\n",
    "\n",
    "Optionally use subword tokenization (Byte-Pair Encoding) if feeding to newer embeddings.\n",
    "\n",
    "Feature engineering\n",
    "\n",
    "Word embeddings: pretrained GloVe / fastText or domain-specific embeddings.\n",
    "\n",
    "Contextual embeddings: optionally feed sentence into a pretrained transformer and then into an LSTM/GRU for downstream tasks (hybrid).\n",
    "\n",
    "Additional features: punctuation counts, all-caps fraction, repeated characters, negation flags, emoji counts, previous-turn metadata (time since last message, user profile flags).\n",
    "\n",
    "Handling out-of-vocabulary and unknowns\n",
    "\n",
    "Use <UNK> token or subword embeddings to avoid KeyErrors.\n",
    "\n",
    "3) NLU model designs using LSTM/GRU\n",
    "\n",
    "A. Intent classification (sequence ‚Üí label)\n",
    "\n",
    "Embedding layer (pretrained or trainable) ‚Üí Bidirectional LSTM or GRU ‚Üí Attention (optional) ‚Üí Dense ‚Üí Softmax.\n",
    "\n",
    "Keras pseudo-architecture:\n",
    "\n",
    "Embedding(input_dim, embed_dim, weights=[pretrained], trainable=False)\n",
    "Bidirectional(GRU(128, return_sequences=True))\n",
    "Attention()  # sums over time steps\n",
    "Dense(64, activation='relu')\n",
    "Dense(num_intents, activation='softmax')\n",
    "\n",
    "\n",
    "Loss: categorical crossentropy.\n",
    "\n",
    "Metrics: accuracy, precision/recall per class (monitor recall for critical classes like suicidal_ideation).\n",
    "\n",
    "B. Sequence labeling (slot filling / NER)\n",
    "\n",
    "Token embedding ‚Üí Bi-LSTM/Bi-GRU ‚Üí CRF or softmax per token.\n",
    "\n",
    "Use BIO tagging.\n",
    "\n",
    "Loss: token-level crossentropy or CRF loss.\n",
    "\n",
    "C. Sentiment / Emotion detection\n",
    "\n",
    "Similar to intent classifier; output multi-label emotions (sad, anxious, angry) using sigmoid if multilabel.\n",
    "\n",
    "D. Crisis (high-recall) classifier\n",
    "\n",
    "A specialized binary classifier trained with heavy class weighting or focal loss to maximize recall (sensitivity).\n",
    "\n",
    "Consider ensemble: lexical-rule triggers (I want to kill myself) plus classifier probability threshold.\n",
    "\n",
    "E. Context handling\n",
    "\n",
    "Use short context window (last N turns). Concatenate utterances or encode each turn then use a turn-level RNN to track dialogue state.\n",
    "\n",
    "F. Why LSTM/GRU?\n",
    "\n",
    "They model sequence order and short-to-medium context efficiently and are lightweight compared to large transformers ‚Äî useful for on-device or low-latency deployments.\n",
    "\n",
    "GRU sometimes preferred for computational efficiency; LSTM slightly better for long-range dependencies. Bidirectional variants improve understanding where appropriate (for offline tasks), but for real-time streaming message you may use unidirectional or include context explicitly.\n",
    "\n",
    "4) Training, validation & evaluation\n",
    "\n",
    "Datasets: Mix of public mental-health conversation datasets (anonymized), in-domain logs (with consent), simulated dialogues, and clinician-annotated examples.\n",
    "\n",
    "Labeling: intents, emotion labels, severity labels, BIO tags for slots. Use multiple annotators; compute inter-annotator agreement (Cohen‚Äôs kappa).\n",
    "\n",
    "Train/val/test splits: stratify to keep rare but critical classes present across splits.\n",
    "\n",
    "Metrics:\n",
    "\n",
    "Intent: accuracy, precision/recall/F1 (report per-class F1)\n",
    "\n",
    "Sentiment/emotion: macro F1\n",
    "\n",
    "Crisis detection: prioritize recall (‚â• 0.95 if possible), report precision, false positives rate\n",
    "\n",
    "Slot filling: token-level F1 / exact match\n",
    "\n",
    "User-level safety: human-evaluation for response appropriateness\n",
    "\n",
    "Calibration: calibrate classifier probabilities (Platt scaling / isotonic) so thresholds for escalation are interpretable.\n",
    "\n",
    "Monitoring: rollout A/B, continuous monitoring for drift, confusion matrices, and false negatives for crisis cases.\n",
    "\n",
    "5) Dialogue manager & response generation\n",
    "\n",
    "Policy:\n",
    "\n",
    "Template-first for safety-critical responses (resource lists, crisis scripts).\n",
    "\n",
    "Retrieval-based for FAQ / knowledge base.\n",
    "\n",
    "Constrained generative model for personalization, but always append/replace unsafe wording via filter.\n",
    "\n",
    "Example flow:\n",
    "\n",
    "If crisis_detector positive ‚Üí immediate safe template: empathetic statement + crisis resources + escalate (offer to connect to human).\n",
    "\n",
    "Else if intent == ask_info ‚Üí retrieve info.\n",
    "\n",
    "Else respond with empathetic paraphrase + follow-up question.\n",
    "\n",
    "Human-in-the-loop: provide ‚Äúescalation‚Äù button; log flagged conversations for clinician review with minimal PII.\n",
    "\n",
    "6) Safety, ethics, and privacy (must have priorities)\n",
    "\n",
    "Crisis first: worst-case safety ‚Äî the system should never miss a high-risk message. Prefer false positives (escalate even when not needed) over false negatives.\n",
    "\n",
    "Human escalation: every high-risk or ambiguous case must have an easy, fast path to a qualified human. Clearly communicate limits of the bot.\n",
    "\n",
    "Informed consent: users must know they‚Äôre talking to a bot, what data is collected, how it‚Äôs used, retained, and when humans will be involved.\n",
    "\n",
    "Data minimization & de-identification: remove names, exact addresses, contact numbers from logs; store only as necessary and encrypt data at rest/in transit.\n",
    "\n",
    "Logging & audit: keep auditable logs for model decisions and escalation triggers for compliance and improvement, but protect privacy.\n",
    "\n",
    "Bias & fairness: test the bot on diverse demographic groups to detect biased responses or differential performance; annotate and correct training data.\n",
    "\n",
    "Transparency & disclaimers: the bot should present itself clearly, provide resource disclaimers, and never claim clinical credentials.\n",
    "\n",
    "Regulatory compliance: HIPAA (US) or GDPR (EU) considerations where applicable ‚Äî ensure data handling, access controls, and data subject rights are respected.\n",
    "\n",
    "Human review & clinical oversight: clinicians should review model outputs periodically and update guidelines; obtain clinical signoff on crisis scripts.\n",
    "\n",
    "Adversarial misuse / hallucination prevention: prevent the generative component from giving medical advice that could be dangerous. Prefer templates for any clinical guidance.\n",
    "\n",
    "7) Deployment & operations\n",
    "\n",
    "Serving:\n",
    "\n",
    "Export models (TorchScript / ONNX) for low-latency serving.\n",
    "\n",
    "Use microservices: NLU service, Dialogue service, Safety service.\n",
    "\n",
    "Scaling: autoscale NLU workers; cache frequent embeddings/responses.\n",
    "\n",
    "Monitoring:\n",
    "\n",
    "Track latency, per-intent accuracy, crisis detection false negatives.\n",
    "\n",
    "Set alarms on drift (sudden increase in unknown tokens, drop in confidence).\n",
    "\n",
    "Model updates:\n",
    "\n",
    "Continuous retraining pipeline with human-in-the-loop labeling.\n",
    "\n",
    "Use canary deployments; roll back if safety metrics degrade.\n",
    "\n",
    "Offline fallback: if models fail, use conservative templates and present human support options.\n",
    "\n",
    "8) Example code snippets (concise)\n",
    "\n",
    "A. spaCy preprocessing snippet:\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [t.text.lower() for t in doc if not t.is_space]\n",
    "    lemmas = [t.lemma_ for t in doc if not t.is_space]\n",
    "    ents = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return {\"tokens\": tokens, \"lemmas\": lemmas, \"ents\": ents}\n",
    "\n",
    "\n",
    "B. Keras-style intent classifier sketch (LSTM):\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_ = Input(shape=(max_len,))\n",
    "x = Embedding(vocab_size, embed_dim, mask_zero=True)(input_)\n",
    "x = Bidirectional(LSTM(128))(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "out = Dense(num_intents, activation='softmax')(x)\n",
    "model = Model(input_, out)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "(Replace with GRU by swapping LSTM for GRU if you prefer lighter compute.)\n",
    "\n",
    "9) Example safety thresholds & rules (practical)\n",
    "\n",
    "Crisis classifier threshold: choose threshold optimizing recall (e.g., threshold 0.35 if calibrated probabilities), but tune on validation with human review.\n",
    "\n",
    "Escalation policy:\n",
    "\n",
    "If crisis flag OR message contains any hard trigger phrases (\"kill myself\", \"end my life\") ‚Üí immediate human escalation and show emergency resources.\n",
    "\n",
    "If sentiment is severely depressed + multiple mentions over N turns ‚Üí prompt escalation.\n",
    "\n",
    "Audit: every escalation creates an immutable case record for clinician review.\n",
    "\n",
    "10) Final notes / trade-offs\n",
    "\n",
    "LSTM/GRU are effective for intent/sentiment/slot tasks with limited compute and small-to-medium datasets. For best accuracy, contextual transformers (BERT) outperform RNNs but are heavier and more prone to hallucination if used for generation.\n",
    "\n",
    "Hybrid approach recommended:\n",
    "\n",
    "spaCy/Stanza for fast syntactic features and NER\n",
    "\n",
    "LSTM/GRU for intent/sentiment/slot models\n",
    "\n",
    "Templates for clinical content and generative models only for non-critical personalization\n",
    "\n",
    "Always prioritize safety, transparency, and clinician oversight in a mental-health application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
